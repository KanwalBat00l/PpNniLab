# Privacy-Preserving Neural Network Inference (PPNNI) System
**Dynamic Orchestration of Secure Inference Servers for Mobile Clients**

## ğŸŒ Overview

This system enables privacy-preserving neural network inference (PPNNI) across distributed devices using a serverâ€“client architecture.

It consists of:

- Server Manager (Kotlin API) â€“ manages the lifecycle of server binaries (model servers).

- Model Server Binary (e.g., OpenCheetah) â€“ runs a privacy-preserving inference protocol for a given neural network model.

- Client Binary (e.g., OpenCheetah client) â€“ connects to the spawned model server to perform encrypted inference on input data (e.g., images).

- Android Mobile App â€“ provides a user-friendly interface to request a server instance and execute secure inference locally via JNI.

**Together, these components allow multiple users to perform secure, on-demand model inference without exposing private input data or model parameters.**


## ğŸ§© System Architecture
<pre>
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Android Mobile App      â”‚
        â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
        â”‚  UI: Select Model + Image    â”‚
        â”‚  ClientManager:              â”‚
        â”‚  - Request Server from API   â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
        â”‚  â”‚ Run Client Binary (JNI) â”‚ â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Server Manager        â”‚
        â”‚   (Kotlin + Ktor REST API)   â”‚
        â”‚                              â”‚
        â”‚  /getServer?model=XYZ        â”‚
        â”‚    â†’ Allocates port, spawns  â”‚
        â”‚      Model Server process    â”‚
        â”‚      and returns IP + port   â”‚
        â”‚                              â”‚
        â”‚  /status                     â”‚
        â”‚    â†’ Returns server states   â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                 â”‚
      â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ModelServer â”‚   â”‚ ModelServer â”‚
â”‚ (Port 6000) â”‚   â”‚ (Port 6001) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

* Each handles one client inference request.
After completion or timeout â†’ terminated.

</pre>


## ğŸ” Workflow


**Step 1 â€” Request Server**

The Android app sends a request to:

`GET /getServer?model=default`


The Server Manager selects a free port (e.g., 6000) and launches a model server process:

`./server 6000`

Server Manager responds with:

```json
{
  "ip": "192.168.1.249",
  "port": 6000,
  "model": "default",
  "status": "ok"
}
```

**Step 2 â€” Run Client Inference**

The mobile app receives the response, and its ClientManager executes:

`NativeBridge.execute("192.168.1.249", 6000, "image.jpg")`

JNI calls the native client binary, which securely connects to the server and performs encrypted inference.

**Step 3 â€” Cleanup and Lifecycle**

Each model server runs only for a fixed lifetime (serverLifetimeMs).

After timeout or process completion, Server Manager kills the server and releases the port.

## ğŸ§° Building and Running
### ğŸ–¥ï¸ Server Manager

```bash
cd ServerManager
./gradlew build
./gradlew run
```

### ğŸ“± Android Client

- Open the Android project in Android Studio.
- Build & install on your device.
- Provide the Server Manager URL (e.g., http://192.168.1.249:8080).
- Choose model and image.
- Run inference.

for More information 
- [ServerManager](/ServerManager/ReadMe.ServerManager.md) 
- [AndroidApp](/AndroidApp/ReadMe.AndroidApp.md) 
- [PPNNI](/PPNNI.Dummy/ReadMe.PPNNI.md) 

### Sample Runs
- ![PPNNI (Server and Client)](/img/ppnni.png)
- ![Server Manager (with PPNNI client only)](/img/servermanager.png)
- ![Android App (communicating with Server Manager)](/img/androidapp.png)